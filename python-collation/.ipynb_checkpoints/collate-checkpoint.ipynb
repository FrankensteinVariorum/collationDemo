{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c66368",
   "metadata": {},
   "source": [
    "# Welcome to *collate.ipynb*!\n",
    "*collate.ipynb* is a visual representation of the *collate.py* script -- designed in Jupyter -- to demostrate the Python collation process. Please keep in mind that the following code normally executes when running the *coll.sh* script. Certain pieces of the original code were changed, shifted, or flatout removed for the sake of clarity.\n",
    "\n",
    "## Run all of the code below in order!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4e4c2",
   "metadata": {},
   "source": [
    "### 1. Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import BinaryIO\n",
    "from collatex import *\n",
    "from xml.dom import pulldom\n",
    "import re\n",
    "# Regular expression operations are very vital, especially when identifying\n",
    "# and cleaning up nodes that appear in the collation texts\n",
    "import glob\n",
    "from datetime import datetime, date\n",
    "# This library is used to obtain the date and time.\n",
    "import sys\n",
    "# The sys library is very important, especially when running 'coll.sh'. \n",
    "# This library is used to controls variables and parts of the Python runtime environment\n",
    "import os\n",
    "# The os library is used to open, write, and manipulate files located on a local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6da60b",
   "metadata": {},
   "source": [
    "### 2. Regex nodes and element identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow()\n",
    "nowStr = str(now)\n",
    "\n",
    "print('test: ', dir(Collation))\n",
    "regexHyphen = re.compile(r'\\S[\\‑‒–—]\\S')\n",
    "regexWhitespace = re.compile(r'\\s+')\n",
    "regexNonWhitespace = re.compile(r'\\S+')\n",
    "regexEmptyTag = re.compile(r'/>$')\n",
    "regexBlankLine = re.compile(r'\\n{2,}')\n",
    "regexLeadingBlankLine = re.compile(r'^\\n')\n",
    "regexPageBreak = re.compile(r'<pb.+?/>', re.DOTALL)\n",
    "RE_MARKUP = re.compile(r'<.+?>', re.DOTALL)\n",
    "RE_WORD_START = re.compile(r'<w ana=\"start\"/>(.+)<lb[^<>]+>')\n",
    "RE_WORD_END = re.compile(r'<w ana=\"end\"/>')\n",
    "# RE_HI_START = re.compile(r'<hi\\ssID.+?/>')\n",
    "# RE_HI_END = re.compile(r'<hi\\seID.+?/>')\n",
    "RE_PARASTART = re.compile(r'<p\\ssID.+?/>')\n",
    "RE_PARAEND = re.compile(r'<p\\seID.+?/>')\n",
    "RE_INCLUDE = re.compile(r'<include.*?/>')\n",
    "RE_HEAD = re.compile(r'<head.*?/>')\n",
    "RE_AB = re.compile(r'<ab.*?/>')\n",
    "# 2018-10-1 ebb: ampersands are apparently not treated in python regex as entities any more than angle brackets.\n",
    "# RE_AMP_NSB = re.compile(r'\\S&amp;\\s')\n",
    "# RE_AMP_NSE = re.compile(r'\\s&amp;\\S')\n",
    "# RE_AMP_SQUISH = re.compile(r'\\S&amp;\\S')\n",
    "# RE_AMP = re.compile(r'\\s&amp;\\s')\n",
    "RE_AMP = re.compile(r'&')\n",
    "RE_LT_AMP = re.compile(r'&amp;')\n",
    "# RE_MULTICAPS = re.compile(r'(?<=\\W|\\s|\\>)[A-Z][A-Z]+[A-Z]*\\s')\n",
    "# RE_INNERCAPS = re.compile(r'(?<=hi\\d\"/>)[A-Z]+[A-Z]+[A-Z]+[A-Z]*')\n",
    "# TITLE_MultiCaps = match(RE_MULTICAPS).lower()\n",
    "# RE_NOTE = re.compile(r'<note[^<]*?>.+?</note>', re.MULTILINE | re.DOTALL)\n",
    "# RE_DEL = re.compile(r'<del[^<\\-]*?>.+?</del>', re.MULTILINE | re.DOTALL)\n",
    "RE_ADDSTART = re.compile(r'<add[^<>]*?>')\n",
    "RE_ADDEND = re.compile(r'</add>')\n",
    "RE_NOTE_START = re.compile(r'<note[^<>]+?sID[^<>]+?>')\n",
    "RE_NOTE_END = re.compile(r'<note[^<>]+?eID[^<>]+?>')\n",
    "RE_DELSTART = re.compile(r'<del[^<>]*?>')\n",
    "RE_DELEND = re.compile(r'</del>')\n",
    "# 2023-05-17 ebb with nlh: We have altered the delSpans thus:\n",
    "# <delSpan spanTo=\"id\"/> as a start marker and a <delSpan anchor=\"id\"/> in the pre-processed msColl for collation.\n",
    "# Before the endpoints were <anchor> elements with only xml:ids,\n",
    "# indistinguishable from the many other anchor elements in the msColl files.\n",
    "# We want to make it possible for these to be seen in the normalized tokens used in the output collation.\n",
    "# so they can be displayed as deleted passages in the variant panels.\n",
    "RE_DELSPAN_START = re.compile(r'<delSpan[^<>]+?spanTo[^<>]+?/>')\n",
    "RE_DELSPAN_END = re.compile(r'<delSpan[^<>]+?anchor[^<>]+?/>')\n",
    "\n",
    "RE_ANCHOR = re.compile(r'<anchor.+?/>')\n",
    "RE_SGA_ADDSTART = re.compile(r'<sga-add[^<>]+?sID[^<>]+?/>')\n",
    "RE_SGA_ADDEND = re.compile(r'<sga-add[^<>]+?eID[^<>]+?/>')\n",
    "RE_MDEL = re.compile(r'<mdel[^<>]*?>[^<>]+?</mdel>')\n",
    "# RE_SHI = re.compile(r'<shi.*?>.+?</shi>')\n",
    "RE_SHI_START = re.compile(r'<shi[^<>]*?>')\n",
    "RE_SHI_END = re.compile(r'</shi>')\n",
    "RE_METAMARK = re.compile(r'<metamark[^<>]*?>.+?</metamark>')\n",
    "RE_HI = re.compile(r'<hi\\s.+?/>')\n",
    "RE_HI_START = re.compile(r'<hi\\s*sID.+?>')\n",
    "RE_HI_END = re.compile(r'<hi\\s*eID.+?>')\n",
    "RE_PB = re.compile(r'<pb.*?/>')\n",
    "RE_SPACE_LB = re.compile(r'([\\w\\s])<lb.*?/>')\n",
    "RE_LB = re.compile(r'<lb.*?/>')\n",
    "# ebb: considered: re.DOTALL ? Probably don't need it b/c these regexes are being performed on tokens.\n",
    "RE_LG = re.compile(r'<lg[^<]*/>')\n",
    "RE_L = re.compile(r'<l\\s[^<]*/>')\n",
    "RE_CIT = re.compile(r'<cit\\s[^<]*/>')\n",
    "RE_QUOTE = re.compile(r'<quote\\s[^<]*/>')\n",
    "RE_OPENQT = re.compile(r'“')\n",
    "RE_CLOSEQT = re.compile(r'”')\n",
    "RE_GAP = re.compile(r'<gap\\s[^<]*/>')\n",
    "# &lt;milestone unit=\"tei:p\"/&gt;\n",
    "RE_sgaPSTART = re.compile(r'<milestone[^<]+?unit=\"tei:p-START.+?/>')\n",
    "RE_sgaPEND = re.compile(r'<milestone[^<]+?unit=\"tei:p-END.+?/>')\n",
    "RE_MILESTONE = re.compile(r'<milestone.+?>')\n",
    "# 2022-07-16 amended 07-31 ebb: Milestone subbing is a special problem: In S-GA paragraphs\n",
    "# are marked w/ milestone elements with @unit=\"tei:p\"\n",
    "# and also milestones are used for other things like \"tei:seg\"\"\n",
    "# So we'll do a regex substitution for the paragraphs first, and THEN move to the other milestones.\n",
    "RE_MOD = re.compile(r'<mod\\s[^<]*/>')\n",
    "RE_MULTI_LEFTANGLE = re.compile(r'<{2,}')\n",
    "RE_MULTI_RIGHTANGLE = re.compile(r'>{2,}')\n",
    "RE_LT_START = re.compile(r'<longToken.*?>')\n",
    "RE_LT_END = re.compile(r'</longToken>')\n",
    "RE_HEAD_START = re.compile(r'<head.*?>')\n",
    "RE_HEAD_END = re.compile(r'</head>')\n",
    "RE_DOTDASH = re.compile(r'\\.–')\n",
    "RE_BIBL = re.compile(r'<bibl.+?>') #Added 2023-07-09 ebb\n",
    "RE_ZONE = re.compile(r'<zone.+?>') #Added 2023-07-30 ebb\n",
    "\n",
    "# RE_DOTDASH captures a period followed by a dash, frequently seen in the S-GA edition, and not a word-dividing hyphen.\n",
    "# 2022-08-08 ebb: I'm currently treating the \"dotdash\" as just a period for normalization to improve alignments.\n",
    "\n",
    "# ebb: RE_MDEL = those pesky deletions of two letters or less that we want to normalize out of the collation, but preserve in the output.\n",
    "\n",
    "# Element types: xml, div, head, p, hi, pb, note, lg, l; comment()\n",
    "# Tags to ignore, with content to keep: xml, comment, anchor\n",
    "# Structural elements: div, p, lg, l\n",
    "# Inline elements (empty) retained in normalization: pb, milestone, xi:include\n",
    "# Inline and block elements (with content) retained in normalization: note, hi, head, ab\n",
    "\n",
    "# GIs fall into one three classes\n",
    "# 2017-05-21 ebb: Due to trouble with pulldom parsing XML comments, I have converted these to comment elements,\n",
    "# 2017-05-21 ebb: to be ignored during collation.\n",
    "# 2017-05-30 ebb: Determined that comment elements cannot really be ignored when they have text nodes (the text is\n",
    "# 2017-05-30 ebb: collated but the tags are not). Decision to make the comments into self-closing elements with text\n",
    "# 2017-05-30 ebb: contents as attribute values, and content such as tags simplified to be legal attribute values.\n",
    "# 2017-05-22 ebb: I've set anchor elements with @xml:ids to be the indicators of collation \"chunks\" to process together\n",
    "ignore = ['sourceDoc', 'xml', 'comment', 'include', 'addSpan', 'handShift', 'damage', 'unclear', 'restore', 'surface', 'zone', 'retrace']\n",
    "blockEmpty = ['p', 'div', 'milestone', 'lg', 'l', 'cit', 'quote', 'bibl']\n",
    "inlineEmpty = ['mod', 'pb', 'sga-add', 'delSpan', 'anchor', 'lb', 'gap', 'hi', 'w', 'ab']\n",
    "inlineContent = ['del-INNER', 'add-INNER', 'metamark', 'shi']\n",
    "inlineVariationEvent = ['head', 'del', 'mdel', 'add', 'note', 'longToken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeSpace(inText):\n",
    "    \"\"\" Replaces all whitespace spans with a newline character for tokenization.\"\"\"\n",
    "    if regexNonWhitespace.search(inText):\n",
    "        return regexWhitespace.sub('\\n', inText)\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09efcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(input_xml):\n",
    "    \"\"\"Process entire input XML document, firing on events\"\"\"\n",
    "    # Start pulling; it continues automatically\n",
    "    doc = pulldom.parse(input_xml)\n",
    "    output = ''\n",
    "    for event, node in doc:\n",
    "        # elements to ignore: xml\n",
    "        if event == pulldom.START_ELEMENT and node.localName in ignore:\n",
    "            continue\n",
    "        # copy comments intact\n",
    "        # if event == pulldom.COMMENT:\n",
    "        #     doc.expandNode(node)\n",
    "        #     output += node.toxml()\n",
    "        # ebb: The following handles our longToken and longToken-style elements:\n",
    "        # complete element nodes surrounded by newline characters to make a long complete token:\n",
    "        if event == pulldom.START_ELEMENT and node.localName in inlineVariationEvent:\n",
    "            doc.expandNode(node)\n",
    "            # 2022-10-25 ebb: The line above may be sending the characters inside the node to be processed twice,\n",
    "            # resulting in the patterns of `&amp;&amp; for a single &amp; or &amp;&amp;quot; and andquot\n",
    "            # output += '\\n' + node.toxml() + '\\n'\n",
    "            # print(node.toxml()); Today, we cannot find a good pre-processing solution, but I hypothesize that\n",
    "            # we might run this node.toxml() output through a sub function to do the equivalent of\n",
    "            # the XSLT replace() we're now doing to handle this problem in post-processing.\n",
    "            output += '\\n' + node.toxml() + '\\n'\n",
    "            # output += '\\n' + node.toxml()\n",
    "        # ebb: Next (below): empty block elements: pb, milestone, lb, lg, l, p, ab, hi,\n",
    "        # We COULD set white spaces around these like this ' ' + node.toxml() + ' '\n",
    "        # but what seems to happen is that the white spaces get added to tokens; they aren't used to\n",
    "        # isolate the markup into separate tokens, which is really what we'd want.\n",
    "        # So, I'm removing the white spaces here.\n",
    "        # NOTE: Removing the white space seems to improve/expand app alignment\n",
    "        # 2022-07-16 ebb: With help from yxj, found that adding \\n to each side of blockEmpty and inlineEmpty elements\n",
    "        # stops the problem of forming tokens that fuse element tags to words.\n",
    "        elif event == pulldom.START_ELEMENT and node.localName in blockEmpty:\n",
    "            output += '\\n' + node.toxml() + '\\n'\n",
    "        # ebb: empty inline elements that do not take surrounding white spaces:\n",
    "        elif event == pulldom.START_ELEMENT and node.localName in inlineEmpty:\n",
    "            output += node.toxml()\n",
    "        # non-empty inline elements: mdel, shi, metamark\n",
    "        elif event == pulldom.START_ELEMENT and node.localName in inlineContent:\n",
    "            output += '\\n' + regexEmptyTag.sub('>', node.toxml())\n",
    "            # output += '\\n' + node.toxml()\n",
    "        elif event == pulldom.END_ELEMENT and node.localName in inlineContent:\n",
    "            output += '</' + node.localName + '>' + '\\n'\n",
    "        # elif event == pulldom.START_ELEMENT and node.localName in blockElement:\n",
    "        #    output += '\\n<' + node.localName + '>\\n'\n",
    "        # elif event == pulldom.END_ELEMENT and node.localName in blockElement:\n",
    "        #    output += '\\n</' + node.localName + '>'\n",
    "        elif event == pulldom.CHARACTERS:\n",
    "            # output += fixToken(normalizeSpace(node.data))\n",
    "            output += normalizeSpace(node.data)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputText):\n",
    "    # 2022-07-16 ebb: Adding newlines here is too late: it just inserts a newline into a token.\n",
    "    # 2022-08-06 ebb: I have rewritten this series of operations using a normalized variable for legibility.\n",
    "    # These need to run in sequence: the order of replacements matters.\n",
    "    # The lower() at the end lowercases all the normalized strings to simplify the comparison.\n",
    "\n",
    "    normalized = RE_METAMARK.sub('', inputText)\n",
    "    normalized = RE_MOD.sub('', normalized)\n",
    "    # 2023-03-16 How about we actually read it this time? <mod> is in the ignore list like anchor, etc, so why are we presuming it's being read?\n",
    "    normalized = RE_GAP.sub('', normalized)\n",
    "    normalized = RE_CLOSEQT.sub('\"', normalized)\n",
    "    normalized = RE_OPENQT.sub('\"', normalized)\n",
    "    normalized = RE_QUOTE.sub('', normalized)\n",
    "    normalized = RE_CIT.sub('', normalized)\n",
    "    normalized = RE_BIBL.sub('', normalized) # Added 2023-07-29 ebb\n",
    "    normalized = RE_L.sub('<l/>', normalized)\n",
    "    normalized = RE_LG.sub('<lg/>', normalized)\n",
    "    normalized = RE_AB.sub('', normalized)\n",
    "    # 2022-08-06 <ab> wraps headings or starts of letters in the print editions\n",
    "    normalized = RE_PARASTART.sub('<p-start/>', normalized)\n",
    "    normalized = RE_PARAEND.sub('<p-end/>', normalized)\n",
    "    normalized = RE_sgaPSTART.sub('<p-start/>', normalized)\n",
    "    normalized = RE_sgaPEND.sub('<p-end/>', normalized)\n",
    "    normalized = RE_MILESTONE.sub('', normalized)\n",
    "    normalized = RE_PB.sub('', normalized)\n",
    "    # 2023-05-17 ebb with nlh: Noting that delSpan needs a way to be expressed\n",
    "    # in the output normalized tokens for the new interface.\n",
    "    # Freshly altered delSpan and anchor in msColl files to delspan...delspan, normalizing in next two lines:\n",
    "    normalized = RE_DELSPAN_START.sub('<del>', normalized)\n",
    "    normalized = RE_DELSPAN_END.sub('</del>', normalized)\n",
    "    normalized = RE_ANCHOR.sub('', normalized)\n",
    "    normalized = RE_LT_AMP.sub('and', normalized)\n",
    "    normalized = RE_AMP.sub('and', normalized)\n",
    "    normalized = RE_WORD_START.sub(' \\\\1', normalized)\n",
    "    # 2023-05-22 ebb and yxj: We must replace WORD_START before the SPACE_LB.\n",
    "    # WORD_START replacement ensures that the normalized token for <w ana='start'/>...<lb/>...<w ana=\"end\"/>\n",
    "    # does not get an added space. We need to ensure that these are treated as single word tokens\n",
    "    # with no space added internally.\n",
    "    normalized = RE_WORD_END.sub(' ', normalized)\n",
    "    normalized = RE_SPACE_LB.sub('\\\\1 ', normalized)\n",
    "    normalized = RE_LB.sub('', normalized)\n",
    "    normalized = RE_ZONE.sub('', normalized)\n",
    "    normalized = RE_NOTE_START.sub('<note>', normalized)\n",
    "    normalized = RE_NOTE_END.sub('</note>', normalized)\n",
    "    normalized = RE_SGA_ADDSTART.sub('', normalized)\n",
    "    normalized = RE_SGA_ADDEND.sub('', normalized)\n",
    "    normalized = RE_ADDSTART.sub('', normalized)\n",
    "    normalized = RE_ADDEND.sub('', normalized)\n",
    "    normalized = RE_DELSTART.sub('<del>', normalized)\n",
    "    normalized = RE_DELEND.sub('</del>', normalized)\n",
    "    normalized = RE_LT_START.sub('', normalized)\n",
    "    normalized = RE_LT_END.sub('', normalized)\n",
    "    normalized = RE_HEAD_START.sub('', normalized)\n",
    "    normalized = RE_HEAD_END.sub('', normalized)\n",
    "    # 2023-06-30 nlh: added space inside <hi/> normalization.\n",
    "    normalized = RE_HI_START.sub('<hi>', normalized) # ebb: 2023-07-30 We used to normalize <hi> as a space, but we're going to try supplying it as a start and end tag for the normalized tokens now.\n",
    "    normalized = RE_HI_END.sub('</hi>', normalized)\n",
    "    # 2022-08-08 ebb: Sometimes <hi> in the print editions seems irrelevant, in highlighting words at\n",
    "    # chapter beginnings. However, it also sometimes indicates emphasis on a word.\n",
    "    # Example: one or two little <hi sID=\"xxx\"/>wives<hi eID=\"novel1_letter4_chapter6_div4_div6_p9_hi1\"/>\n",
    "    # On analysis of <hi> and <shi> in the print and SG-A editions, it is difficult to distinguish\n",
    "    # meaningful highlights from conventional superscripts/underlining, so it seems best to ignore it in normalization,\n",
    "    # or return to the source texts and add markup to distinguish passages giving emphasis.\n",
    "    #  normalized = RE_SHI.sub('', normalized)\n",
    "    normalized = RE_SHI_START.sub('', normalized)\n",
    "    normalized = RE_SHI_END.sub('', normalized)\n",
    "    # 2022-08-08 ebb: <shi> elements mark briefly highlighted words or passages in the S-GA edition.\n",
    "    # The highlights themselves are not usually significant, but the text inside must be preserved for comparison.\n",
    "    # Example: <shi rend=\"underline\">should be</shi>\n",
    "    # Previously, we were eliminating these passages entirely from the normalization, which was a serious error!\n",
    "    # We have not been considering highlighting or emphasis <hi> or <shi> as a significant difference in the normalization.\n",
    "    # 2023-03-16 ebb: We have moved mdel to inlineVariationEvent, and do not want to normalize its token, so we are commenting out the next line.\n",
    "    # normalized = RE_MDEL.sub('', normalized)\n",
    "    # 2022-08-08 ebb: <mdel> elements are tiny struck-out characters in the S-GA edition.\n",
    "    # We do not think these are significant for comparison with the other editions, so we normalize them out.\n",
    "    normalized = RE_DOTDASH.sub('. ', normalized)\n",
    "    normalized = RE_INCLUDE.sub('', normalized)\n",
    "    normalized = RE_MULTI_RIGHTANGLE.sub('>', normalized)\n",
    "    normalized = RE_MULTI_LEFTANGLE.sub('<', normalized)\n",
    "    normalized = re.sub(r'^\\s+', '', normalized) # 2023-06-26 yxj: remvoe the space at the beginning\n",
    "    normalized = re.sub(r'\\s+$', '', normalized) # 2023-06-26 yxj: remvoe the space at the end\n",
    "    normalized = normalized.lower()\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processToken(inputText):\n",
    "    return {\"t\": inputText + ' ', \"n\": normalize(inputText)}\n",
    "\n",
    "\n",
    "def processWitness(inputWitness, id):\n",
    "    return {'id': id, 'tokens': [processToken(token) for token in inputWitness]}\n",
    "\n",
    "\n",
    "def tokenize(inputFile):\n",
    "    return regexLeadingBlankLine.sub('\\n', regexBlankLine.sub('\\n', extract(inputFile))).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFiles(f1818, f1823, fThomas, f1831, fMS):\n",
    "    with open(f1818, 'rb') as f1818file, \\\n",
    "            open(f1823, 'rb') as f1823file, \\\n",
    "            open(fThomas, 'rb') as fThomasfile, \\\n",
    "            open(f1831, 'rb') as f1831file, \\\n",
    "            open(fMS, 'rb') as fMSfile:\n",
    "        f1818_tokenlist = processWitness(tokenize(f1818file), 'f1818')\n",
    "        fThomas_tokenlist = processWitness(tokenize(fThomasfile), 'fThomas')\n",
    "        f1823_tokenlist = processWitness(tokenize(f1823file), 'f1823')\n",
    "        f1831_tokenlist = processWitness(tokenize(f1831file), 'f1831')\n",
    "        fMS_tokenlist = processWitness(tokenize(fMSfile), 'fMS')\n",
    "        return [f1818_tokenlist, f1823_tokenlist, fThomas_tokenlist, f1831_tokenlist, fMS_tokenlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # For this example, chunk is set to C18. In other words, when this function runs, it will process Chunk 18.\n",
    "    chunk = \"C18\"\n",
    "    # Normally, this chunk variable would be assigned an agrument variable given by the shell script:\n",
    "    # chunk = sys.argv[1]\n",
    "    for f1818 in glob.glob('../collationChunks/' + chunk + '/input/1818_fullFlat_*'):\n",
    "        try:\n",
    "            collChunk = f1818.split(\"fullFlat_\", 1)[1]\n",
    "                # ebb: above gets C30.xml for example\n",
    "                # matchStr = matchString.split(\".\", 1)[0]\n",
    "                # ebb: above strips off the file extension\n",
    "\n",
    "            f1823 = '../collationChunks/' + chunk + '/input/1823_fullFlat_' + collChunk\n",
    "            fThomas = '../collationChunks/' + chunk + '/input/Thomas_fullFlat_' + collChunk\n",
    "            f1831 = '../collationChunks/' + chunk + '/input/1831_fullFlat_' + collChunk\n",
    "            fMS = '../collationChunks/' + chunk + '/input/msColl_' + collChunk\n",
    "            # 2023-05-17 ebb: **Before we begin the tokenizing**, run a XSLT pre-processing pass:\n",
    "            # Revise delSpan anchor elements and remove newlines from inlineVariationEvent elements so these hold together as long tokens:\n",
    "\n",
    "            tokenLists = tokenizeFiles(f1818, f1823, fThomas, f1831, fMS)\n",
    "            print(tokenLists)\n",
    "            # 2022-11-14 yxj: For easier doing unit testing,\n",
    "            # can we import 4 filenames instead of only 1 into tokenizeFiles()?\n",
    "\n",
    "            collation_input = {\"witnesses\": tokenLists}\n",
    "            outputFile = open('../collationChunks/' + chunk + '/output/Collation_' + chunk + '-partway.xml', 'w', encoding='utf-8')\n",
    "                \n",
    "            # table = collate(collation_input, output='tei', segmentation=True)\n",
    "            # table = collate(collation_input, segmentation=True, layout='vertical')\n",
    "            table = collate(collation_input, output='xml', segmentation=True)\n",
    "            outputFile.close()\n",
    "\n",
    "        except IOError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3bc3ed",
   "metadata": {},
   "source": [
    "### Locked and loaded!\n",
    "When the script runs, the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
